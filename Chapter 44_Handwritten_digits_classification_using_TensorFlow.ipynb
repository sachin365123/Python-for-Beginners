{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V59sfvcaw-o3"
      },
      "source": [
        "# Handwritten digits classification using TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/tensorflow/examples.git"
      ],
      "metadata": {
        "id": "ioeAB356xFPB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow\n",
        "!ypip install tensorflow==1.13.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZMqliqnByzX6",
        "outputId": "f3b2fa0b-398d-47a1-89d6-14bac5622a89"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.7.0\n",
            "Uninstalling tensorflow-2.7.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.7.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.7.0\n",
            "Collecting tensorflow==1.13.2\n",
            "  Downloading tensorflow-1.13.2-cp37-cp37m-manylinux1_x86_64.whl (92.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.7 MB 32 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.19.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 56.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.42.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.37.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.4.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 37.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.3.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.10.0.2)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.2) (1.5.2)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.13.2 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x"
      ],
      "metadata": {
        "id": "ZmF-ES_s0UI_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLVW-jkQz7GI",
        "outputId": "3e506bc2-2859-43ba-942a-dd908d6fec91"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4dkn7VIv0NIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vloJaMjxsBe",
        "outputId": "55083084-99a7-421c-eb4b-5e873dee3124"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-0fd756a69215>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pszxfYnpw-o8"
      },
      "source": [
        "Putting all the concepts we have learned so far, we will see how can use tensorflow to\n",
        "build a neural network to recognize handwritten digits. If you are playing around deep\n",
        "learning off late then you must have come across MNIST dataset. It is being called the hello\n",
        "world of deep learning. \n",
        "\n",
        "It consists of 55,000 data points of handwritten digits (0 to 9).\n",
        "In this section, we will see how can we use our neural network to recognize the\n",
        "handwritten digits and also we will get hang of tensorflow and tensorboard.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkXNqXp8w-o9"
      },
      "source": [
        "## Import required libraries\n",
        "\n",
        "As a first step, let us import all the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yIUyJ09Jw-o-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UaDx3Kv9w-o_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR2oy66bw-pA"
      },
      "source": [
        "## Load the Dataset\n",
        "\n",
        "In the below code, \"data/mnist\" implies the location where we store the MNIST dataset.\n",
        "one_hot=True implies we are one-hot encoding the labels (0 to 9):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoDq2rRRw-pA",
        "outputId": "0845ef69-1708-4b0f-be04-4a0a02b3ddc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        }
      ],
      "source": [
        "mnist = input_data.read_data_sets(\"data/mnist\", one_hot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po33f2m9w-pB"
      },
      "source": [
        "Let's check what we got in our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5ZUk_elw-pB",
        "outputId": "b964bd57-ecec-4b2a-cc19-f787f6ed04a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of images in training set (55000, 784)\n",
            "No of labels in training set (55000, 10)\n",
            "No of images in test set (10000, 784)\n",
            "No of labels in test set (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "print(\"No of images in training set {}\".format(mnist.train.images.shape))\n",
        "print(\"No of labels in training set {}\".format(mnist.train.labels.shape))\n",
        "\n",
        "print(\"No of images in test set {}\".format(mnist.test.images.shape))\n",
        "print(\"No of labels in test set {}\".format(mnist.test.labels.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt-0evwNw-pC"
      },
      "source": [
        "We have 55,000 images in the training set and each image is of size 784 and we have 10 labels which are actually 0 to 9. Similarly, we have 10000 images in the test set.\n",
        "\n",
        "Now we plot one image to see how it looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "ZrtnKPUbw-pD",
        "outputId": "4346f1d8-b564-49de-daec-c2310ef063dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fadef385e90>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "img1 = mnist.train.images[0].reshape(28,28)\n",
        "plt.imshow(img1, cmap='Greys')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHFeKQ6Lw-pD"
      },
      "source": [
        "## Define the number of neurons in each layer\n",
        "\n",
        "We build a 4 layer neural network with 3 hidden layers and 1 output layer. As the size of\n",
        "the input image is 784. We set the num_input to 784 and since we have 10 handwritten\n",
        "digits (0 to 9), We set 10 neurons in the output layer. We define the number of neurons in\n",
        "each layer as follows,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "StwELnPiw-pE"
      },
      "outputs": [],
      "source": [
        "#number of neurons in input layer\n",
        "num_input = 784  \n",
        "\n",
        "#number of neurons in hidden layer 1\n",
        "num_hidden1 = 512  \n",
        "\n",
        "#number of neurons in hidden layer 2\n",
        "num_hidden2 = 256  \n",
        "\n",
        "#number of neurons in hidden layer 3\n",
        "num_hidden_3 = 128  \n",
        "\n",
        "#number of neurons in output layer\n",
        "num_output = 10  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmtLmtA1w-pE"
      },
      "source": [
        "## Defining placeholders\n",
        "\n",
        "As we learned, we first need to define the placeholders for input and output. Values for\n",
        "the placeholders will be feed at the run time through feed_dict:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "A8JOTNR_w-pE"
      },
      "outputs": [],
      "source": [
        "with tf.name_scope('input'):\n",
        "    X = tf.placeholder(\"float\", [None, num_input])\n",
        "\n",
        "with tf.name_scope('output'):\n",
        "    Y = tf.placeholder(\"float\", [None, num_output])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBzo9SAqw-pF"
      },
      "source": [
        "Since we have a 4 layer network, we have 4 weights and 4 baises. We initialize our weights\n",
        "by drawing values from the truncated normal distribution with a standard deviation of\n",
        "0.1. \n",
        "\n",
        "Remember, the dimensions of the weights matrix should be a number of neurons in the\n",
        "previous layer x number of neurons in the current layer. For instance, the dimension of\n",
        "weight matrix w3 should be the number of neurons in the hidden layer 2 x number of\n",
        "neurons in hidden layer 3.\n",
        "\n",
        "We often define all the weights in a dictionary as given below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL8k-xNMw-pF",
        "outputId": "187e615d-1765-4935-9e08-e8d89f6bf229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        }
      ],
      "source": [
        "with tf.name_scope('weights'):\n",
        "    \n",
        "        weights = {\n",
        "        'w1': tf.Variable(tf.truncated_normal([num_input, num_hidden1], stddev=0.1),name='weight_1'),\n",
        "        'w2': tf.Variable(tf.truncated_normal([num_hidden1, num_hidden2], stddev=0.1),name='weight_2'),\n",
        "        'w3': tf.Variable(tf.truncated_normal([num_hidden2, num_hidden_3], stddev=0.1),name='weight_3'),\n",
        "        'out': tf.Variable(tf.truncated_normal([num_hidden_3, num_output], stddev=0.1),name='weight_4'),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFIZgVOdw-pF"
      },
      "source": [
        "The dimension of bias should be a number of neurons in the current layer. For instance, the\n",
        "dimension of bias b2 is the number of neurons in the hidden layer 2. We set the bias value\n",
        "as constant 0.1 in all layers:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xAe3Au_1w-pG"
      },
      "outputs": [],
      "source": [
        "with tf.name_scope('biases'):\n",
        "\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.constant(0.1, shape=[num_hidden1]),name='bias_1'),\n",
        "        'b2': tf.Variable(tf.constant(0.1, shape=[num_hidden2]),name='bias_2'),\n",
        "        'b3': tf.Variable(tf.constant(0.1, shape=[num_hidden_3]),name='bias_3'),\n",
        "        'out': tf.Variable(tf.constant(0.1, shape=[num_output]),name='bias_4')\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59_ZPZ-Uw-pG"
      },
      "source": [
        "## Forward Propagation\n",
        "\n",
        "Now, we define the forward propagation operation. We use relu activations in all layers\n",
        "and in the last layer we use sigmoid activation as defined below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-rqIm1SEw-pG"
      },
      "outputs": [],
      "source": [
        "with tf.name_scope('Model'):\n",
        "    \n",
        "    with tf.name_scope('layer1'):\n",
        "        layer_1 = tf.nn.relu(tf.add(tf.matmul(X, weights['w1']), biases['b1']) )   \n",
        "    \n",
        "    with tf.name_scope('layer2'):\n",
        "        layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['w2']), biases['b2']))\n",
        "        \n",
        "    with tf.name_scope('layer3'):\n",
        "        layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['w3']), biases['b3']))\n",
        "        \n",
        "    with tf.name_scope('output_layer'):\n",
        "         y_hat = tf.nn.sigmoid(tf.matmul(layer_3, weights['out']) + biases['out'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkrNie-nw-pG"
      },
      "source": [
        "## Compute Loss and Backpropagate\n",
        "\n",
        "\n",
        "\n",
        "Next, we define our loss function. We use softmax cross-entropy as our loss\n",
        "function. Tensorflow\n",
        "provides tf.nn.softmax_cross_entropy_with_logits() function for computing the\n",
        "softmax cross entropy loss. It takes the two parameters as inputs logits and labels.\n",
        "\n",
        "* logits implies the logits predicted by our network. That is, y_hat\n",
        "\n",
        "* labels imply the actual labels. That is, true labels y\n",
        "\n",
        "\n",
        "We take mean of the loss using tf.reduce_mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1Xu-G9Yw-pH",
        "outputId": "7d1e1d9b-8c83-4c5c-e483-d37668be1135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-19-37005de64a0c>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with tf.name_scope('Loss'):\n",
        "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_hat,labels=Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs98IWt7w-pH"
      },
      "source": [
        "Now, we need to minimize the loss using backpropagation. Don't worry! We don't have to\n",
        "calculate derivatives of all the weights manually. Instead, we can use tensorflow's\n",
        "optimizer. In this section, we use Adam optimizer. It is a variant of gradient descent\n",
        "optimization technique we learned in the previous chapter. In the next chapter, we will\n",
        "dive into detail and see how exactly all the Adam and several other optimizers work. For\n",
        "now, let's say we use Adam optimizer as our backpropagation algorithm,\n",
        "\n",
        "\n",
        "tf.train.AdamOptimizer() requires the learning rate as input. So we set 1e-4 as the learning rate and we minimize the loss with minimize() function. It computes the gradients and updates the parameters (weights and biases) of our network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MMRlRgIew-pH"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ1Ku1fIw-pH"
      },
      "source": [
        "##  Compute Accuracy\n",
        "\n",
        "We calculate the accuracy of our model as follows. \n",
        "\n",
        "\n",
        "* y_hat denotes the predicted probability for each class by our model. Since we have 10 classes we will have 10 probabilities. If the probability is high at position 7, then it means that our network predicts the input image as digit 7 with high probability.  tf.argmax() returns the index of the largest value. Thus, tf.argmax(y_hat,1) gives the index where the probability is high. Thus, if the probability is high at index 7, then it returns 7\n",
        "<br>\n",
        "\n",
        "\n",
        "* Y denotes the actual labels and it is the one hot encoded values. That is, it consists of zeros everywhere except at the position of the actual image where it consists of 1. For instance, if the input image is 7, then Y has 0 at all indices except at index 7 where it has 1. Thus, tf.argmax(Y,1) returns 7 because that is where we have high value i.e 1. \n",
        "\n",
        "\n",
        "Thus, tf.armax(y_hat,1) gives the predicted digit and tf.argmax(Y,1) gives us the actual digit. \n",
        "\n",
        "tf.equal(x, y) takes x and y as inputs and returns the truth value of (x == y) element-wise. Thus, correct_pred = tf.equal(predicted_digit,actual_digit) consists of True where the actual and predicted digits are same and False where the actual and predicted digits are not the same. We convert the boolean values in correct_pred into float using tensorflow's cast operation. That is, tf.cast(correct_pred, tf.float32). After converting into float values, take the average using tf.treduce_mean().\n",
        "\n",
        "Thus, tf.reduce_mean(tf.cast(correct_pred, tf.float32)) gives us the average correct predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bz2X5Cj7w-pI"
      },
      "outputs": [],
      "source": [
        "with tf.name_scope('Accuracy'):\n",
        "    \n",
        "    predicted_digit = tf.argmax(y_hat, 1)\n",
        "    actual_digit = tf.argmax(Y, 1)\n",
        "    \n",
        "    correct_pred = tf.equal(predicted_digit,actual_digit)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46fcD0bLw-pI"
      },
      "source": [
        "## Create Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edEEn92xw-pI"
      },
      "source": [
        "We can also visualize how the loss and accuracy of our model change during several\n",
        "iterations in tensorboard. So, we use tf.summary() to get the summary of the variable.\n",
        "Since the loss and accuracy are scalar variables, we use tf.summary.scalar() to store the summary as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2Vu8qOZw-pI",
        "outputId": "5191e918-b54d-408c-da66-0bfb34e4f8db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Loss_1:0' shape=() dtype=string>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "tf.summary.scalar(\"Accuracy\", accuracy)\n",
        "\n",
        "tf.summary.scalar(\"Loss\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHD_18YPw-pJ"
      },
      "source": [
        "Next, we merge all the summaries we use in our graph using tf.summary.merge_all(). We merge all summaries because when we have many summaries running and storing them would become inefficient, so we merge all the summaries and run them once in our session instead of running multiple times. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pmW6rWAYw-pJ"
      },
      "outputs": [],
      "source": [
        "merge_summary = tf.summary.merge_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KnBtQkcw-pJ"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIRa2B78w-pJ"
      },
      "source": [
        "Now it is time to train our model. As we learned, first we need to initialize all the variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XUGZSe2ew-pJ"
      },
      "outputs": [],
      "source": [
        "init = tf.global_variables_initializer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-yFKAQSw-pJ"
      },
      "source": [
        "Define the batch size and number of iterations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1k2YIDoBw-pK"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "num_iterations = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iygna918w-pK"
      },
      "source": [
        "Start the tensorflow session and perform training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuF7bJaYw-pK",
        "outputId": "27734528-6bd3-4d67-b992-13e0d01be993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0, Loss: 2.2976858615875244, Accuracy: 0.0859375\n",
            "Iteration: 100, Loss: 1.8103277683258057, Accuracy: 0.8046875\n",
            "Iteration: 200, Loss: 1.6543406248092651, Accuracy: 0.8125\n",
            "Iteration: 300, Loss: 1.6026771068572998, Accuracy: 0.9140625\n",
            "Iteration: 400, Loss: 1.5614373683929443, Accuracy: 0.9140625\n",
            "Iteration: 500, Loss: 1.5513249635696411, Accuracy: 0.921875\n",
            "Iteration: 600, Loss: 1.5469648838043213, Accuracy: 0.9453125\n",
            "Iteration: 700, Loss: 1.5958802700042725, Accuracy: 0.859375\n",
            "Iteration: 800, Loss: 1.53548002243042, Accuracy: 0.9453125\n",
            "Iteration: 900, Loss: 1.5333045721054077, Accuracy: 0.9453125\n"
          ]
        }
      ],
      "source": [
        "with tf.Session() as sess:\n",
        "\n",
        "    #run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    #save the event files\n",
        "    summary_writer = tf.summary.FileWriter('./graphs', graph=tf.get_default_graph())\n",
        "\n",
        "    #train for some n number of iterations\n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        #get batch of data according to batch size\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        \n",
        "        #train the network\n",
        "        sess.run(optimizer, feed_dict={\n",
        "            X: batch_x, Y: batch_y\n",
        "            })\n",
        "\n",
        "        #print loss and accuracy on every 100th iteration\n",
        "        if i % 100 == 0:\n",
        "            \n",
        "            #compute loss, accuracy and summary\n",
        "            batch_loss, batch_accuracy,summary = sess.run(\n",
        "                [loss, accuracy, merge_summary], feed_dict={X: batch_x, Y: batch_y}\n",
        "                )\n",
        "\n",
        "            #store all the summaries\n",
        "            summary_writer.add_summary(summary, i)\n",
        "\n",
        "\n",
        "            print('Iteration: {}, Loss: {}, Accuracy: {}'.format(i,batch_loss,batch_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIHzFikWw-pK"
      },
      "source": [
        "As you may observe, the loss decreases and the accuracy increases over the training iterations. Now that we have learned how to build the neural network using tensorflow, in the next section we will see how can we visualize the computational graph of our model in tensorboard. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "2.05 Handwritten digits classification using TensorFlow.ipynb",
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}